{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Imputation and Classification\n",
    "With and without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.impute import KNNImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. We replace missing values in the dataset by calculating the average of their K-Nearest Neighbors,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train=load_from_tsfile_to_dataframe(\"DodgerLoopGame\\DodgerLoopGame_TRAIN.ts\", return_separate_X_and_y=True)\n",
    "X_test, Y_test=load_from_tsfile_to_dataframe(\"DodgerLoopGame\\DodgerLoopGame_TEST.ts\", return_separate_X_and_y=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_2 = X_train[['dim_0']]\n",
    "\n",
    "X_train_2['dim_0'] = X_train_2['dim_0'].apply(lambda x: np.array(x))\n",
    "X_train_3 = np.stack(X_train_2['dim_0'].to_numpy()) #stack \"puts one above the other\" like rebuilding the column\n",
    "\n",
    "X_test_full = X_test[['dim_0']]\n",
    "X_test_2 = X_test_full.sample(frac=0.8, random_state=1)\n",
    "X_val_2 = X_test_full.drop(X_test_2.index) #here we separate the validation X from Test\n",
    "\n",
    "Y_val_3 = Y_test[X_val_2.index] #here we got the validation Y\n",
    "\n",
    "X_val_2['dim_0'] = X_val_2['dim_0'].apply(lambda x: np.array(x))\n",
    "X_val_3 = np.stack(X_val_2['dim_0'].to_numpy())  #here we finish processing X for validation\n",
    "\n",
    "Y_train_3 = np.array(Y_train)\n",
    "\n",
    "Y_test_3 = Y_test[X_test_2.index] #here we got the testing Y\n",
    "\n",
    "X_test_2['dim_0'] = X_test_2['dim_0'].apply(lambda x: np.array(x))\n",
    "X_test_3 = np.stack(X_test_2['dim_0'].to_numpy())   #here we finish processing X for testing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset X_train has a NaN count of: 65\n",
      "the dataset X_test has a NaN count of: 168\n",
      "the dataset X_val has a NaN count of: 104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train_3_Nan_count=np.isnan(X_train_3).sum()\n",
    "X_test_3_Nan_count=np.isnan(X_test_3).sum()\n",
    "X_val_3_Nan_count=np.isnan(X_val_3).sum()\n",
    "\n",
    "print(\"the dataset X_train\", \"has a NaN count of:\", X_train_3_Nan_count)\n",
    "print(\"the dataset X_test\", \"has a NaN count of:\", X_test_3_Nan_count)\n",
    "print(\"the dataset X_val\", \"has a NaN count of:\", X_val_3_Nan_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68079.92048929445, 67924.91828385394, 67892.17623438784, 67860.97500834748, 67826.9485593052, 67825.97852645494, 67820.23292734692, 67810.70749156731, 67809.49043624672, 67804.01824031709, 67801.96125073028, 67801.70612179363, 67804.27893124646, 67803.42967927226, 67805.04860768361, 67805.84103655763, 67802.69760536042, 67802.70983963653, 67801.24019326191]\n",
      "the optimal K for imputation is 19\n"
     ]
    }
   ],
   "source": [
    "distances_for_k=[] #we start from an empty distance K, we'll see how every K performs\n",
    "for k in range(1,20):\n",
    "    imputer= KNNImputer(n_neighbors=k)\n",
    "    X_val_3b= X_val_3\n",
    "\n",
    "    X_val_3b=imputer.fit_transform(X_val_3b)\n",
    "    pdistances=np.sum(pdist(X_val_3b, \"euclidean\"))\n",
    "\n",
    "    #pdistances=np.sum(cdist(X_val_3b, X_val_3b, \"euclidean\"))\n",
    "    distances_for_k.append(pdistances)\n",
    "\n",
    "print(distances_for_k)\n",
    "k=(np.argmin(distances_for_k)+1) \n",
    "imputer= KNNImputer(n_neighbors=k)\n",
    "X_val_4= X_val_3\n",
    "X_val_4=imputer.fit_transform(X_val_4)\n",
    "X_train_4=imputer.fit_transform(X_train_3)\n",
    "X_test_4=imputer.fit_transform(X_test_3)\n",
    "\n",
    "print(\"the optimal K for imputation is\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN without sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 288)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_val_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL LABELS \n",
      "\n",
      "['1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '2' '2' '2' '2'\n",
      " '2' '2' '2' '2' '2' '2' '2' '2' '2' '2']\n",
      "k 1 , acc 0.8214285714285714\n",
      "k 2 , acc 0.7142857142857143\n",
      "k 3 , acc 0.6428571428571429\n",
      "k 4 , acc 0.4642857142857143\n",
      "k 5 , acc 0.4642857142857143\n",
      "k 6 , acc 0.4642857142857143\n",
      "k 7 , acc 0.4642857142857143\n",
      "k 8 , acc 0.4642857142857143\n",
      "k 9 , acc 0.4642857142857143\n",
      "k 10 , acc 0.42857142857142855\n",
      "k 11 , acc 0.4642857142857143\n",
      "k 12 , acc 0.39285714285714285\n",
      "k 13 , acc 0.4642857142857143\n",
      "k 14 , acc 0.35714285714285715\n",
      "k 15 , acc 0.42857142857142855\n",
      "k 16 , acc 0.5\n",
      "Optimal K: 1 ,with an accuracy of 0.8214285714285714\n",
      "PREDICTED LABELS \n",
      "\n",
      "['1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '2' '2' '1' '1'\n",
      " '2' '1' '2' '2' '1' '2' '2' '2' '2' '1']\n"
     ]
    }
   ],
   "source": [
    "def predictor(x_train_array, y_train_array, x_test_array, k): \n",
    "    predicted_labels=[]\n",
    "    for test_row in x_test_array:\n",
    "        euclidean_dist=np.empty(len(x_train_array)) #why not a list with append? lists have given me problems in previous attempts and dimentionality when turning them to numpy\n",
    "        #so better to try and \"initialize it\" as a np array\n",
    "        for i in range(len(x_train_array)):\n",
    "            euclidean_dist[i]= (np.sum((test_row - x_train_array[i]) ** 2))**0.5\n",
    "            #now we need to find the closest\n",
    "            #according to np doccumentation, np.argsort \"Returns the indices that would sort an array.\"\n",
    "        ordered_indices=np.argsort(euclidean_dist)\n",
    "        ordered_indices= ordered_indices[:k]\n",
    "        y_train_labels=y_train_array[ordered_indices]\n",
    "        label_options, label_options_counts = np.unique(y_train_labels, return_counts=True)\n",
    "        elected_label=label_options[np.argmax(label_options_counts)]\n",
    "        predicted_labels.append(elected_label)\n",
    "    return np.array(predicted_labels)\n",
    "\n",
    "print(\"REAL LABELS\", \"\\n\")\n",
    "print(Y_val_3)\n",
    "\n",
    "optimal_neighbors_k=1 #placeholder to initialize\n",
    "accuracy=float(0.000) #placeholder to initialize\n",
    "for k in range(1,17):\n",
    "    predicted=predictor(X_train_3, Y_train_3, X_val_3, k)\n",
    "    accuracy_loop = np.mean(predicted == Y_val_3)\n",
    "    print(\"k\", k,\", acc\", accuracy_loop)\n",
    "    if accuracy_loop > accuracy:\n",
    "        accuracy = accuracy_loop\n",
    "        optimal_neighbors_k = k\n",
    "print(\"Optimal K:\", optimal_neighbors_k, \",with an accuracy of\",accuracy )\n",
    "predicted=predictor(X_train_3, Y_train_3, X_val_3, optimal_neighbors_k)\n",
    "print(\"PREDICTED LABELS\", \"\\n\")\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL LABELS \n",
      "\n",
      "['1' '1' '2' '1' '1' '1' '2' '1' '2' '1' '2' '1' '2' '2' '1' '2' '2' '1'\n",
      " '1' '1' '1' '2' '1' '1' '1' '1' '2' '1' '2' '1' '2' '1' '2' '2' '1' '1'\n",
      " '1' '2' '2' '1' '1' '2' '1' '1' '1' '2' '1' '2' '2' '2' '2' '2' '2' '1'\n",
      " '2' '2' '1' '1' '1' '1' '2' '2' '1' '2' '2' '2' '1' '1' '1' '2' '1' '2'\n",
      " '2' '1' '1' '1' '2' '2' '2' '2' '2' '2' '2' '1' '1' '2' '1' '2' '1' '1'\n",
      " '2' '1' '1' '2' '2' '2' '1' '1' '1' '1' '2' '1' '2' '1' '2' '2' '1' '2'\n",
      " '1' '1']\n",
      "k 1 , acc 0.8727272727272727\n",
      "k 2 , acc 0.7181818181818181\n",
      "k 3 , acc 0.7818181818181819\n",
      "k 4 , acc 0.7\n",
      "k 5 , acc 0.7454545454545455\n",
      "k 6 , acc 0.6909090909090909\n",
      "k 7 , acc 0.7090909090909091\n",
      "k 8 , acc 0.6909090909090909\n",
      "k 9 , acc 0.7363636363636363\n",
      "k 10 , acc 0.6454545454545455\n",
      "k 11 , acc 0.7272727272727273\n",
      "k 12 , acc 0.6181818181818182\n",
      "k 13 , acc 0.7090909090909091\n",
      "k 14 , acc 0.5545454545454546\n",
      "k 15 , acc 0.7\n",
      "k 16 , acc 0.5272727272727272\n",
      "Optimal K: 1 ,with an accuracy of 0.8727272727272727\n",
      "PREDICTED LABELS \n",
      "\n",
      "['1' '1' '2' '1' '1' '1' '2' '1' '2' '1' '2' '1' '1' '2' '1' '2' '1' '1'\n",
      " '1' '1' '1' '1' '1' '1' '1' '1' '2' '1' '2' '1' '2' '1' '2' '1' '1' '1'\n",
      " '1' '1' '2' '1' '1' '2' '1' '1' '1' '2' '1' '1' '2' '2' '1' '2' '2' '1'\n",
      " '2' '2' '1' '1' '1' '1' '2' '2' '1' '2' '2' '2' '1' '1' '1' '1' '1' '2'\n",
      " '2' '1' '1' '1' '2' '2' '2' '2' '2' '2' '2' '1' '1' '1' '1' '1' '1' '1'\n",
      " '1' '1' '1' '2' '1' '1' '1' '1' '1' '1' '2' '1' '2' '2' '2' '2' '1' '2'\n",
      " '1' '1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"REAL LABELS\", \"\\n\")\n",
    "print(Y_test_3)\n",
    "\n",
    "optimal_neighbors_k=1 #placeholder to initialize\n",
    "accuracy=float(0.000) #placeholder to initialize\n",
    "for k in range(1,17):\n",
    "    predicted=predictor(X_train_3, Y_train_3, X_test_3, k)\n",
    "    accuracy_loop = np.mean(predicted == Y_test_3)\n",
    "    print(\"k\", k,\", acc\", accuracy_loop)\n",
    "    if accuracy_loop > accuracy:\n",
    "        accuracy = accuracy_loop\n",
    "        optimal_neighbors_k = k\n",
    "print(\"Optimal K:\", optimal_neighbors_k, \",with an accuracy of\",accuracy )\n",
    "predicted=predictor(X_train_3, Y_train_3, X_test_3, optimal_neighbors_k)\n",
    "print(\"PREDICTED LABELS\", \"\\n\")\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL LABELS \n",
      "\n",
      "['1' '1' '2' '1' '1' '1' '2' '1' '2' '1' '2' '1' '2' '2' '1' '2' '2' '1'\n",
      " '1' '1' '1' '2' '1' '1' '1' '1' '2' '1' '2' '1' '2' '1' '2' '2' '1' '1'\n",
      " '1' '2' '2' '1' '1' '2' '1' '1' '1' '2' '1' '2' '2' '2' '2' '2' '2' '1'\n",
      " '2' '2' '1' '1' '1' '1' '2' '2' '1' '2' '2' '2' '1' '1' '1' '2' '1' '2'\n",
      " '2' '1' '1' '1' '2' '2' '2' '2' '2' '2' '2' '1' '1' '2' '1' '2' '1' '1'\n",
      " '2' '1' '1' '2' '2' '2' '1' '1' '1' '1' '2' '1' '2' '1' '2' '2' '1' '2'\n",
      " '1' '1']\n",
      "k 1 , acc 0.9\n",
      "k 2 , acc 0.8\n",
      "k 3 , acc 0.85\n",
      "k 4 , acc 0.75\n",
      "k 5 , acc 0.75\n",
      "k 6 , acc 0.75\n",
      "k 7 , acc 0.85\n",
      "k 8 , acc 0.8\n",
      "k 9 , acc 0.85\n",
      "k 10 , acc 0.75\n",
      "k 11 , acc 0.75\n",
      "k 12 , acc 0.7\n",
      "k 13 , acc 0.8\n",
      "k 14 , acc 0.6\n",
      "k 15 , acc 0.7\n",
      "k 16 , acc 0.5\n",
      "Optimal K: 1 ,with an accuracy of 0.9\n",
      "PREDICTED LABELS \n",
      "\n",
      "['1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '2' '2' '2' '2' '2' '1' '2' '2'\n",
      " '2' '1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"REAL LABELS\", \"\\n\")\n",
    "print(Y_test_3)\n",
    "\n",
    "optimal_neighbors_k=1 #placeholder to initialize\n",
    "accuracy=float(0.000) #placeholder to initialize\n",
    "for k in range(1,17):\n",
    "    predicted=predictor(X_train_3, Y_train_3, X_train_3, k)\n",
    "    accuracy_loop = np.mean(predicted == Y_train_3)\n",
    "    print(\"k\", k,\", acc\", accuracy_loop)\n",
    "    if accuracy_loop > accuracy:\n",
    "        accuracy = accuracy_loop\n",
    "        optimal_neighbors_k = k\n",
    "print(\"Optimal K:\", optimal_neighbors_k, \",with an accuracy of\",accuracy )\n",
    "predicted=predictor(X_train_3, Y_train_3, X_train_3, optimal_neighbors_k)\n",
    "print(\"PREDICTED LABELS\", \"\\n\")\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression\n",
    "Decision Tree Regression model from scratch using the Residual Sum of Squares (RSS) as the split quality criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:  # we create the class with the attributes indicated, split_feature, threshold, predicted_value\n",
    "  def __init__(self, predicted_value,split_feature, threshold):\n",
    "    self.split_feature, self.threshold, self.predicted_value= split_feature, threshold, predicted_value\n",
    "    self.right, self.left = None, None\n",
    "\n",
    "\n",
    "class DecisionTreeRegressor:  #the general regressor\n",
    "  def __init__(self,X, Y):\n",
    "    self.X, self.Y = X, Y\n",
    "  #what do I need?,---->for predict_sample I need predict---->for predict I need predict_sample--->for fit i need build_tree\n",
    "  #for build_tree i need TreeNode, find_best_split---->for find_best_split I need calculate_rss first\n",
    "  #therefore I should begin with calculate_rss,then find_best_split, then build_tree, then figure out both Predict functions\n",
    "\n",
    "  def calculate_rss(self, y_left , y_right ):\n",
    "    var_left, var_right= np.var(y_left), np.var(y_right)\n",
    "    #Return the sum of the variances as RSS\n",
    "    rss= var_left+var_right\n",
    "    return rss\n",
    "\n",
    "\n",
    "  def find_best_split(self, X, Y):  #this takes our X and Y, doesnt need any other parameters as we initialize rss, feature and threshold\n",
    "    best_rss, best_feature, best_threshold = float('inf'), None, None\n",
    "    #now for every column, we take all their values and define to which side of the threshold we are and create a list of indexes\n",
    "    #that list of idexes we will pass then to get labels Y and see where they are compared to the threshold\n",
    "    #For each feature in X: Iterate through unique threshold values in the feature\n",
    "    #so iterate features and iterate threshold\n",
    "\n",
    "\n",
    "    for feature in range(X.shape[1]): #for every column (For each feature in X)\n",
    "      #print(X[:, feature])\n",
    "      for threshold in X[:, feature]: #we take every value\n",
    "        #will fill with 1s where it goes\n",
    "        #first we initialize\n",
    "        left_indices=np.zeros(X.shape[0])\n",
    "        right_indices=np.zeros(X.shape[0])\n",
    "        for j in range(X.shape[0]):\n",
    "          if X[j, feature] < threshold:\n",
    "            left_indices[j]= 1\n",
    "          else:\n",
    "            right_indices[j]= 1\n",
    "\n",
    "        #now we have created two lists telling us if each row is in left or right\n",
    "        #now we have the index for the x that go to the left or the right, now we take that index to redirect the Y\n",
    "        left_values, right_values=Y[left_indices==1], Y[right_indices==1]\n",
    "        if (len(left_values)<1 or len(right_values)<1):\n",
    "          pass\n",
    "        else:\n",
    "          rss=self.calculate_rss(left_values, right_values)\n",
    "          if rss<best_rss:\n",
    "            best_rss, best_feature, best_threshold=rss, feature, threshold\n",
    "    return best_rss, best_feature, best_threshold #independently of what happes, returns best rss, feature and threshold after iterating through each feature with every threshold\n",
    "  \n",
    "\n",
    "  \n",
    "  def build_tree(self, X, Y, depth =0):\n",
    "    #now we take a node, if we are at max depth return mean\n",
    "    #then we find how are we goin to split, by calling find_best_split\n",
    "    #calling this function will return the best split, which we will use for left and right\n",
    "    #then all of this \"subsets\" will take a node and restart the process\n",
    "    Node= TreeNode(predicted_value=None, split_feature=None, threshold=None)      #Attributes empty because our Fit will fill them will fill them\n",
    "    #If stopping criteria are met ( min samples or max depth ): Set the node 's predicted_value to the mean of y. Return the node .\"Use a stopping criterion based on either a maximum depth or a minimum number of samples in a node.\" Instructions ask to do either, #therefore this case will use depth\n",
    "    placeholder_max_depth=10\n",
    "    if depth >= placeholder_max_depth:\n",
    "      Node.predicted_value=np.mean(Y)    #note for the future self: numpy mean unlike panda doesnt deal with NaN, how should i deal with them in the future?\n",
    "      #print(Node.predicted_value) \n",
    "      return Node\n",
    "    else:\n",
    "      pass\n",
    "    #Find the best feature and threshold by calling find_best_split.\n",
    "    best_rss, best_feature, best_threshold= self.find_best_split(X, Y)\n",
    "    if best_feature is None:\n",
    "      Node.predicted_value=np.mean(Y)\n",
    "      #print(Node.predicted_value) \n",
    "      return Node\n",
    "  \n",
    "    Node.split_feature, Node.threshold = best_feature, best_threshold #Set the node split_feature and threshold to the selected values\n",
    "    #with [:, column] we take all the rows and the specified column, which means now we'll take all our input points and compare\n",
    "    #the desired feature against the threshold\n",
    "    left_indices, right_indices=(X[:, Node.split_feature] < Node.threshold), (X[:, Node.split_feature] >= Node.threshold)\n",
    "    #now we have the indexes, we just assign them to Y\n",
    "    left_values, right_values=Y[left_indices], Y[right_indices]\n",
    "    X_rec_left, Y_rec_left=X[left_indices],Y[left_indices]\n",
    "    X_rec_right, Y_rec_right=X[right_indices],Y[right_indices]\n",
    "    #print(\"Node of depth\", depth+1)\n",
    "    #now recursivity\n",
    "\n",
    "    Node.left= self.build_tree(X_rec_left, Y_rec_left, depth =depth+1)\n",
    "    Node.right= self.build_tree(X_rec_right, Y_rec_right, depth =depth+1)\n",
    "    return Node\n",
    "\n",
    "  def fit(self, X, Y):\n",
    "    self.tree= self.build_tree(X, Y, depth =0) \n",
    "\n",
    "  def predict(self, X):\n",
    "    predictions=[]\n",
    "    for sample in X:\n",
    "      #print(\"sample\",sample)\n",
    "      Predicted_Sample=self.predict_sample(sample, self.tree)\n",
    "      #print(\"p sample\",Predicted_Sample)\n",
    "      predictions.append(Predicted_Sample)\n",
    "    predictions = np.array(predictions).reshape(-1, 1) # the instructions ask to return an array\n",
    "    return predictions\n",
    "\n",
    "  def predict_sample(self, sample, Node):\n",
    "    if Node.predicted_value != None:\n",
    "        return Node.predicted_value\n",
    "    else:\n",
    "      if sample[Node.split_feature] < Node.threshold:\n",
    "        return self.predict_sample(sample, Node.left)\n",
    "      else:  \n",
    "        return self.predict_sample(sample, Node.right)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = pd.read_csv(\"iris.data\", delimiter=\",\", header=None)\n",
    "\n",
    "#we can replace manually, but thinking ahead this could be troublesome with a bigger list of options\n",
    "\n",
    "labels = np.unique(iris_data.iloc[:,-1])\n",
    "labels_coded = dict(zip(labels, range(1, len(labels) + 1)))\n",
    "iris_data.iloc[:,-1] = iris_data.iloc[:,-1].map(labels_coded)\n",
    "\n",
    "Y=iris_data.iloc[:, -1]\n",
    "X = iris_data.drop(iris_data.columns[-1], axis=1) #axis 1 means drop column, axis 0 would be row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#changed the normalizing method in order to improve results, not sure if helped\n",
    "#def scale(x,mean, std):\n",
    "#    z= (x-mean)/std\n",
    "#    return z\n",
    "#for xn in X.columns:\n",
    "#    mean, std= X[xn].mean(), X[xn].std()\n",
    "#    X[xn] = scale(X[xn], mean, std)\n",
    "\n",
    "\n",
    "def min_max_scale(x,min_x, max_x):\n",
    "    z = (x-min_x)/(max_x-min_x)\n",
    "    return z\n",
    "\n",
    "for xn in X.columns:\n",
    "    min_val, max_val = X[xn].min(), X[xn].max()\n",
    "    X[xn] = min_max_scale(X[xn], min_val, max_val)\n",
    "\n",
    "X_Train=X.sample(frac=0.8,  random_state=1) #i use 1 as in the documentation example\n",
    "X_Test = X.drop(X_Train.index)\n",
    "Y_Train = Y.loc[X_Train.index]\n",
    "Y_Test = Y.drop(Y_Train.index)\n",
    "X_Train=X_Train.values\n",
    "Y_Train=Y_Train.values\n",
    "X_Test=X_Test.values\n",
    "Y_Test=Y_Test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "145    3\n",
      "146    3\n",
      "147    3\n",
      "148    3\n",
      "149    3\n",
      "Name: 4, Length: 150, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: 0.9883748005374975\n",
      "1 [1.]\n",
      "1 [1.]\n",
      "1 [1.]\n",
      "1 [1.]\n",
      "1 [1.]\n",
      "1 [1.]\n",
      "1 [1.]\n",
      "1 [1.]\n",
      "1 [1.]\n",
      "1 [1.]\n",
      "2 [2.]\n",
      "2 [2.]\n",
      "2 [2.]\n",
      "2 [2.03571429]\n",
      "2 [2.03571429]\n",
      "2 [2.03571429]\n",
      "2 [2.03571429]\n",
      "2 [2.03571429]\n",
      "2 [2.03571429]\n",
      "2 [2.03571429]\n",
      "2 [2.]\n",
      "2 [2.03571429]\n",
      "2 [2.03571429]\n",
      "3 [2.97222222]\n",
      "3 [2.97222222]\n",
      "3 [2.]\n",
      "3 [2.03571429]\n",
      "3 [2.03571429]\n",
      "3 [2.97222222]\n",
      "3 [2.97222222]\n"
     ]
    }
   ],
   "source": [
    "DTR = DecisionTreeRegressor(X_Train, Y_Train)\n",
    "DTR.fit(X_Train, Y_Train)\n",
    "\n",
    "test_predicted=DTR.predict(X_Test)\n",
    "\n",
    "valid_mse = np.mean((Y_Test - test_predicted)**2)\n",
    "#print(test_predicted)\n",
    "print(\"ERROR:\",valid_mse)\n",
    "\n",
    "#print(np.shape(Y_Test), np.shape(test_predicted))\n",
    "\n",
    "for i in range(len(Y_Test)):\n",
    "    print(Y_Test[i], test_predicted[i] )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
